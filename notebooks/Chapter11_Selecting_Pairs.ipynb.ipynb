{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pairs Selection\n",
    "\n",
    "### This notebook is the implementation part for Chapter 1. \n",
    "\n",
    "The workflow in this notebook is roughly:\n",
    "\n",
    "1. Load data\n",
    "2. Apply PCA to reduce dimensions of data and normalize principal compoinents using a Scaler\n",
    "3. Fit data into OPTICS, a clustering algorithm\n",
    "4. Compute cluster assignments & possible number of pairs\n",
    "5. Visualize clusters (2D & 3D) using tSNE\n",
    "6. Apply statistical criteria to filter out pairs, namely cointegration tests, checking the Hurst exponent for spreads, checking the half-life of spreads and checking the number of times the spread crosses its mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, read and preview data\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import OPTICS\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import json\n",
    "from scipy.stats import linregress\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from hurst import compute_Hc\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "with open('../data/ch0_variables.pkl', 'rb') as f:\n",
    "    df, df_returns = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of PCA\n",
    "\n",
    "In our current dataset, we have ~1000 data points per ticker. This makes it difficult to cluster these assets based on their features, therefore we would like a representation of the price series of each ticker with less dimensions. This approach also loosely represents the decomposition of risk factors of price series in theoretical finance. Further explanation about the relationship between dimensionality reduction and risk factors can be found in the thesis itself.\n",
    "\n",
    "Existing literature proposes that 5 principal components are a decent choice in terms of explained variance and addition of further principal components has marginally low benefits after that point (Sarmento, 2019). More dimensions will result in more explained variance for PCA but will cripple our efforts of clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to returns data\n",
    "\n",
    "pca = PCA(n_components=5, svd_solver='auto', random_state=42)\n",
    "pca.fit(df_returns)\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "for i in range(len(explained_variance)):\n",
    "    print(f\"Variance explained by PC{i+1}: {explained_variance[i]:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal components can have varying magnitudes which will skew the results of our clustering algorithm, therefore we normalize the principal components using a standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize PCA components using Standard Scaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(pca.components_.T)\n",
    "print(f\"Means of PCs after scaling: {scaler.mean_}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(scaler.mean_)), scaler.mean_)\n",
    "plt.title('Means of PCs after Scaling')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.ylabel('Mean')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Standard deviations of PCs after scaling: {scaler.scale_}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(scaler.scale_)), scaler.scale_)\n",
    "plt.title('Standard Deviations (Scales) of PCs after Scaling')\n",
    "plt.xlabel('Principal Component Index')\n",
    "plt.ylabel('Standard Deviation (Scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "### Why OPTICS?\n",
    "\n",
    "The most common clustering algorithms assume gaussian distributions along the cluster members (which is a strong assumption about our data we're not yet ready to make), are not well choices for handling outliers or noisy data (which we assume we have because of the nature of financial data), require number of clusters or cluster definitions to be specified beforehand (which we don't know) or require intervention of the developer/trader/investor through a termination criterion which might cause unnecessary bias in a trading scenario (which we're trying to avoid altogether with the ML approach throughout this whole experiment). Based on these requirements, we can eliminate all partition-based clustering algorithms (e.g. k-means) and hierarchical clustering algorithms.\n",
    "\n",
    "Based on the above criteria, density-based algorithms seem like a good fit. The most popular algorithm in this group is DBSCAN, but DBSCAN requires the developer to specify the number of clusters and the radius of the cluster beforehand. That leads us to the choice of OPTICS, which relies on a more flexible (and more compute heavy) approach where the algorithm itself decides on the number of clusters and the radius of the cluster based on a single parameter by the developer, the minimum number of members in the cluster. This deems the algorithm **almost** parameterless. OPTICS also does a good job of classifying outliers (or members not assigned to any cluster), which plays well into our pairs trading framework: It helps us filter out ETFs with no cluster meaning no similarity in price trends with any other members in the group.\n",
    "\n",
    "We proceed by fitting the results of our PCA into OPTICS. We inspect the number of clusters, number of ETFs assigned to a cluster and the number of possible pair combinations to evaluate in the following section. We also visualize the results of the clustering algorithm using 2D and 3D t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster PCs of returns using OPTICS\n",
    "\n",
    "optics = OPTICS(min_samples=3, max_eps=5, xi=0.05, metric='euclidean', cluster_method='xi')\n",
    "optics.fit(X)\n",
    "labels = optics.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(\"Number of clusters after OPTICS (excluding outliers): %d\" % n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign clusters to ETFs\n",
    "\n",
    "clustered_series_all = pd.Series(index=df_returns.columns, data=labels.flatten())\n",
    "clustered_series = clustered_series_all[clustered_series_all != -1]\n",
    "\n",
    "print(f\"Number of ETFs assigned to a cluster: {len(clustered_series)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cluster counts and pairs to evaluate\n",
    "\n",
    "counts = clustered_series.value_counts()\n",
    "counts.sort_index(inplace=True)\n",
    "for i in counts.index:\n",
    "    print(f\"Cluster {i}: {counts[i]} ETFs\")\n",
    "\n",
    "print('Average cluster size: ', np.mean(counts))\n",
    "print(\"Pairs to evaluate: %d\" % (counts * (counts - 1) / 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cluster counts\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(counts.index, counts.values)\n",
    "plt.title('Cluster Member Counts')\n",
    "plt.yticks(np.arange(0, len(counts), 1))\n",
    "plt.xlabel('ETFs within cluster', size=12)\n",
    "plt.ylabel('Cluster Id', size=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scaled PCs to 2d t-SNE for visualization\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2d t-SNE visualization with cluster labels\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "colors = ['gray', 'blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', \n",
    "               'darkblue', 'darkgreen', 'darkred', \n",
    "               'darkcyan', 'darkmagenta', 'olive',\n",
    "               'lightblue', 'lightgreen', 'salmon', 'lightcyan', \n",
    "               'blueviolet', 'lightyellow', \n",
    "               'orange', 'purple', 'brown']\n",
    "for i in range(len(X_tsne)):\n",
    "    plt.scatter(X_tsne[i, 0], X_tsne[i, 1], color=colors[labels[i] + 1], alpha=0.5, s=100)\n",
    "    if labels[i] == -1:\n",
    "        continue\n",
    "    plt.text(X_tsne[i, 0], X_tsne[i, 1], df_returns.columns[i], fontsize=8, fontweight='bold', horizontalalignment='center', verticalalignment='bottom')\n",
    "plt.title('t-SNE Visualization with Tickers')\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit scaled PCs to 3d t-SNE for visualization\n",
    "\n",
    "tsne = TSNE(n_components=3, perplexity=30, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3d t-SNE visualization with cluster labels\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "colors = ['gray', 'blue', 'green', 'red', 'cyan', 'magenta', 'yellow', 'black', \n",
    "               'darkblue', 'darkgreen', 'darkred', \n",
    "               'darkcyan', 'darkmagenta', 'olive',\n",
    "               'lightblue', 'lightgreen', 'salmon', 'lightcyan', \n",
    "               'blueviolet', 'lightyellow', \n",
    "               'orange', 'purple', 'brown']\n",
    "for i in range(len(X_tsne)):\n",
    "    ax.scatter(X_tsne[i, 0], X_tsne[i, 1], X_tsne[i, 2], color=colors[labels[i] + 1], alpha=0.5, s=100)\n",
    "    if labels[i] == -1:\n",
    "        continue\n",
    "    ax.text(X_tsne[i, 0], X_tsne[i, 1], X_tsne[i, 2], df_returns.columns[i], fontsize=8, fontweight='bold', horizontalalignment='center', verticalalignment='bottom')\n",
    "ax.set_title('3D t-SNE Visualization with Tickers')\n",
    "ax.set_xlabel('t-SNE Component 1')\n",
    "ax.set_ylabel('t-SNE Component 2')\n",
    "ax.set_zlabel('t-SNE Component 3')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize price series for all clusters\n",
    "\n",
    "for cluster in range(len(counts)):\n",
    "    symbols = list(clustered_series[clustered_series==cluster].index)\n",
    "    means = np.log(df[symbols].mean())\n",
    "    series = np.log(df[symbols]).sub(means)\n",
    "    series.plot(figsize=(10,5), title='ETFs Time Series for Cluster %d' % (cluster))\n",
    "    plt.ylabel('Normalized log prices', size=12)\n",
    "    plt.xlabel('Date', size=12)\n",
    "    num_ticks = 5\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    plt.xticks(df.index[::len(df.index)//num_ticks])\n",
    "    plt.xticks(rotation=90, ha='right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep for Criteria Evaluation\n",
    "\n",
    "The last step in choosing pairs is filtering them through a sequence of statistical criteria. To achieve that, we need to compile all pairs to evaluate in a single list to iterate over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clusters and tickers\n",
    "\n",
    "cluster_dict = defaultdict(list)\n",
    "pairs_dict = defaultdict(list)\n",
    "pairs_to_eval = []\n",
    "\n",
    "for i in range(len(clustered_series)):\n",
    "    cluster_dict[int(clustered_series.iloc[i])].append(clustered_series.index[i])\n",
    "\n",
    "print(f\"Clusters and their tickers:\\n{json.dumps(cluster_dict, indent=4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairs in each cluster\n",
    "\n",
    "for k, v in cluster_dict.items():\n",
    "    pair_combinations = list(combinations(v, 2))\n",
    "    pairs_dict[k] = pair_combinations\n",
    "\n",
    "print(f\"Pairs in each cluster:\\n{json.dumps(pairs_dict, indent=4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all pairs in a single list\n",
    "\n",
    "for k, v in pairs_dict.items():\n",
    "    pairs_to_eval += pairs_dict[k]\n",
    "\n",
    "print(\"All pair combinations:\")\n",
    "for i in pairs_to_eval:\n",
    "    print(f\"{i[0]} <--> {i[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Criteria to Filter Pairs\n",
    "\n",
    "Currently, we have possible pair combinations based on our clustering algorithm. In this section, we apply a sequence of tests to determine whether there are viable pairs to trade in the pairs identified by our clustering algorithm. We rely on 4 separate tests:\n",
    "1. **Cointegration:** Two time series being cointegrated tells us that these two time series may exhibit short-term fluctuations but they share a common stochastic trend in the long run. In statistical terms: Despite the two time series not being stationary individually, there exists a linear combination of the two series that is stationary. We use the Augmented Dickey-Fuller Test to estimate whether two time series are cointegrated. This involves constructing a linear regression and then checking for stationarity in the residuals.\n",
    "\n",
    "2. **Hurst exponent:** Hurst exponent is an estimator for the persistence of time series data. In short, a Hurst exponent lower than 0.5 implies that the series exhibit mean-reverting tendencies.\n",
    "\n",
    "3. **Half-life:** The half-life of time series indicates the number of units of time required to revert to its mean. We use half-life as a proxy measure for liquidity and frequency of trades. We construct an OLS estimator, lag the residuals and then use the intercept in the half-life formula to estimate.\n",
    "\n",
    "4. **Mean-crossing frequency:** Pretty self-explanatory, this is the number of times the residuals (which is the spread in our case) crosses its own mean. Financially, it makes sense that this statistic has a lower bound, otherwise we wouldn't be able to execute trades.\n",
    "\n",
    "Thresholds we use are:\n",
    "1. Cointegration p-value must be smaller than 0.05 for statistical significance.\n",
    "2. Hurst exponent must be smaller than 0.5 to exhibit mean reverting properties.\n",
    "3. Half-life must be smaller than 260 days (average number of working days on a non-leap year) to ensure enough liquidity in our trades and that we close positions regularly.\n",
    "4. The spread of a pair must cross its own mean at least 48 times in the training period to average a trade per month (assuming training data is 4 years).\n",
    "\n",
    "### A pair is deemed valid only if it satisfies all of the above criteria. We find that there are 19 such pairs in our initial list of 166 pairs to evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_valid_pairs = pairs_to_eval.copy()\n",
    "\n",
    "cointegration_p_val_threshold = 0.1 # Assumed to be cointegrated if smaller than 0.05\n",
    "hurst_exponent_threshold = 0.5 # Assumed to be mean reverting if smaller than 0.5\n",
    "half_life_threshold = 260 # Assumed to revert to mean in maximum 1 year (260 working days) if smaller than 260\n",
    "mean_crosses_threshold = 48 # Assumed to revert to mean 48 times in 4 years (on average once every month) if greater than 48\n",
    "\n",
    "for pair in pairs_to_eval:\n",
    "\n",
    "    price_series_1 = df[pair[0]]\n",
    "    price_series_2 = df[pair[1]]\n",
    "\n",
    "    # Check for cointegration using ADF test\n",
    "\n",
    "    slope, intercept, _, _, _ = linregress(price_series_1, price_series_2)\n",
    "    residuals = price_series_2 - (slope * price_series_1 + intercept)\n",
    "    cointegration_result = adfuller(residuals, autolag='AIC')\n",
    "\n",
    "    # Check for mean reversion using Hurst exponent\n",
    "\n",
    "    H_val, _, _ = compute_Hc(residuals)\n",
    "\n",
    "    # Check for stationarity using half-life of mean-reverting process\n",
    "\n",
    "    lagged_residuals = np.roll(residuals, 1)\n",
    "    lagged_residuals[0] = 0\n",
    "    delta_residuals = residuals - lagged_residuals\n",
    "    lagged_residuals_with_intercept = np.vstack([lagged_residuals, np.ones(len(lagged_residuals))]).T\n",
    "    model = OLS(delta_residuals, lagged_residuals_with_intercept).fit()\n",
    "    half_life = -np.log(2) / model.params.iloc[0]\n",
    "\n",
    "    # Check for mean reversion frequency using mean crossings\n",
    "\n",
    "    delta_residuals_mean = residuals - np.mean(residuals)\n",
    "    mean_crossings = sum(1 for i, _ in enumerate(delta_residuals_mean) if (i + 1 < len(delta_residuals_mean)) if ((delta_residuals_mean.iloc[i] * delta_residuals_mean.iloc[i + 1] < 0) or (delta_residuals_mean.iloc[i] == 0)))\n",
    "\n",
    "    # Compute valid pairs that satisfy all criteria. remove remaining pairs\n",
    "\n",
    "    if cointegration_result[1] < cointegration_p_val_threshold and H_val < hurst_exponent_threshold and half_life < half_life_threshold and mean_crossings > mean_crosses_threshold:\n",
    "        print(f\"{pair[0]} and {pair[1]} are a valid pair.\")\n",
    "    else:\n",
    "        criteria_valid_pairs.remove(pair)\n",
    "    \n",
    "print(f\"Found {len(criteria_valid_pairs)} valid pairs in total.\")\n",
    "\n",
    "with open('../data/ch1_valid_pairs.pkl', 'wb') as f:\n",
    "    pickle.dump(criteria_valid_pairs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
